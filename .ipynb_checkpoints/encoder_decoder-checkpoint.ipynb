{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import cv2\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, BatchNormalization, LeakyReLU, Dropout\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "from keras.utils import plot_model\n",
    "\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.utils import get_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base URL and filename for CelebA dataset\n",
    "base_url = 'https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/celeba.zip'\n",
    "file_name = 'celeba.zip'\n",
    "\n",
    "# Download the CelebA dataset zip file\n",
    "path = get_file(file_name, base_url, extract=False)\n",
    "\n",
    "# Load the images from the zip file\n",
    "with zipfile.ZipFile(path, 'r') as zip_ref:\n",
    "    zip_ref.extractall('/content/')  # Extract the zip file to a specific directory\n",
    "\n",
    "# Define the directory path where the images are extracted\n",
    "data_dir = '/content/img_align_celeba/'\n",
    "\n",
    "# Load images using TensorFlow's load_img function\n",
    "images = []\n",
    "for i in range(1, 202599):  # Assuming the dataset contains 202,599 images\n",
    "    file_path = data_dir + f'{i:06d}.jpg'  # File naming convention in CelebA dataset\n",
    "    img = load_img(file_path)\n",
    "    img = img.resize((64, 64))  # Resize the images to desired size\n",
    "    img = np.array(img)\n",
    "    images.append(img)\n",
    "\n",
    "# Convert the list of images into a numpy array\n",
    "images = np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sampling random images from our whole dataset for training and evaluation purporses\n",
    "import random\n",
    "images_ds = random.sample(list(images),60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_ds = np.array(images_ds)\n",
    "type(images_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test = train_test_split(images_ds, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing as pre\n",
    "\n",
    "x_train = X_train.reshape(-1, 1)\n",
    "scaler = pre.MinMaxScaler()\n",
    "x_train_norm = scaler.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = X_test.reshape(-1, 1)\n",
    "x_test_norm = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_batch = np.split(X_train,30)\n",
    "x_test_batch = np.split(X_test,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train_batch = np.array(x_train_batch)\n",
    "x_test_batch = np.array(x_test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_norm = x_train_batch /255\n",
    "x_test_norm = x_test_batch /255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input shape and latent dimension\n",
    "latent_dim = 2\n",
    "input_shape = (64, 64, 3)\n",
    "# Encoder network\n",
    "inputs = Input(shape=input_shape)\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(inputs)\n",
    "x = Conv2D(32, (3, 3), activation='relu', strides=(2, 2), padding='same')(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "shape_before_flattening = K.int_shape(x)\n",
    "x = Flatten()(x)\n",
    "z_mean = Dense(latent_dim)(x)\n",
    "z_log_var = Dense(latent_dim)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling function\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim))\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "# Reparameterization trick\n",
    "z = Lambda(sampling)([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder network\n",
    "decoder_input = Input(K.int_shape(z)[1:])\n",
    "x = Dense(np.prod(shape_before_flattening[1:]), activation='relu')(decoder_input)\n",
    "x = Reshape(shape_before_flattening[1:])(x)\n",
    "x = Conv2DTranspose(128, (2, 2), activation='relu', padding='same', )(x)\n",
    "x = Conv2DTranspose(64, (2, 2), activation='relu', padding='same', strides=(2, 2))(x)\n",
    "x = Conv2DTranspose(32, (2, 2), activation='relu', padding='same', )(x)\n",
    "x = Conv2DTranspose(16, (2, 2), activation='relu', padding='same', )(x)\n",
    "x = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the VAE model\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "decoder = Model(decoder_input, x, name='decoder')\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='vae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64, 64, 3)]       0         \n",
      "_________________________________________________________________\n",
      "encoder (Functional)         [(None, 2), (None, 2), (N 1385828   \n",
      "_________________________________________________________________\n",
      "decoder (Functional)         (None, 64, 64, 3)         961187    \n",
      "=================================================================\n",
      "Total params: 2,347,015\n",
      "Trainable params: 2,347,015\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the VAE loss function\n",
    "from keras.losses import mse\n",
    "reconstruction_loss = mse(K.flatten(inputs), K.flatten(outputs))\n",
    "reconstruction_loss *= input_shape[0] * input_shape[1] * input_shape[2]\n",
    "kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=1)\n",
    "B = 1000   \n",
    "vae_loss = K.mean(B * reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.add_metric(kl_loss, name=\"kl_loss\")\n",
    "vae.add_metric(reconstruction_loss, name=\"reconstruction_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.compile(optimizer='adam')\n",
    "vae.fit(x_train_batch, epochs=30, batch_size=30, validation_data=(x_test_batch, None))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
